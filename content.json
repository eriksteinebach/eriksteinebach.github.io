{"meta":{"title":"NET, DevOps, Architecture, Azure, Erik Steinebach","subtitle":"Erik Steinebach","description":"I am a digital nomad, a .NET developer, a software architect, a development process consultant and a new technology enthusiast. Originally from The Netherlands, but currently living in Lima, Peru.","author":"Erik Steinebach","url":"http://eriksteinebach.com"},"pages":[{"title":"About","date":"2018-05-06T17:52:23.903Z","updated":"2018-05-06T17:52:23.903Z","comments":true,"path":"about/index.html","permalink":"http://eriksteinebach.com/about/index.html","excerpt":"","text":"Hi, I am Erik Steinebach. Thank you for your interest. As a freelance consultant, I provide software development, consultancy, coaching and mentoring services to companies. Learn more about me or have a look at my public CV on LinkedIn, and contact me if you want more information. We can always have a chat. My focus is on: .NET, C# Azure Cloud Microsoft SQL Server development and performance optimization Secure Event Driven system Architecture (Microservices, EDA and CQRS) Agile software development practices like SCRUM, Kanban, Continuous Delivery/Deployment and DevOps I like to work remotely, but can always come to your office(s) as well - whether in Peru, or around the world. You can contact me at: Email: info@eriksteinebach.nl Twitter: eriksteinebach LinkedIn: eriksteinebach Github: eriksteinebach"}],"posts":[{"title":"Javascript script for Application Insights","slug":"application-insights-for-javascript-script","date":"2018-06-11T01:33:40.000Z","updated":"2018-06-11T01:50:41.885Z","comments":true,"path":"2018/06/10/application-insights-for-javascript-script/","link":"","permalink":"http://eriksteinebach.com/2018/06/10/application-insights-for-javascript-script/","excerpt":"","text":"There are a lot of scripts available on how to implement Application Insights on the server side. Also, for a lot of different ways to customize the client. But I had a hard time finding how to customize Application Insights on the client side. So below I include the script we use with some customizations you can use to log what you want to get out of appInsights.123456789101112131415161718192021222324252627282930var appInsights = window.appInsights || function (config) &#123; function r(config) &#123; t[config] = function () &#123; var i = arguments; t.queue.push(function () &#123; t[config].apply(t, i) &#125;) &#125; &#125; var t = &#123; config: config &#125;, u = document, e = window, o = 'script', s = u.createElement(o), i, f; for (s.src = config.url || '//az416426.vo.msecnd.net/scripts/a/ai.0.js', u.getElementsByTagName(o)[0].parentNode.appendChild(s), t.cookie = u.cookie, t.queue = [], i = ['Event', 'Exception', 'Metric', 'PageView', 'Trace', 'Ajax']; i.length;) r('track' + i.pop()); return r('setAuthenticatedUserContext'), r('clearAuthenticatedUserContext'), config.disableExceptionTracking || (i = 'onerror', r('_' + i), f = e[i], e[i] = function (config, r, u, e, o) &#123; var s = f &amp;&amp; f(config, r, u, e, o); return s !== !0 &amp;&amp; t['_' + i](config, r, u, e, o), s &#125;), t&#125;(&#123; instrumentationKey: '@Model.AppInsightsInstrumentationKey'&#125;);window.appInsights = appInsights;appInsights.queue.push(function () &#123; @if (!String.IsNullOrWhiteSpace(User.Identity.Name)) &#123; &lt;text&gt; var appInsightsUserName = '@User.Identity.Name.Replace(\"\\\\\", \"\\\\\\\\\")'; appInsights.setAuthenticatedUserContext(appInsightsUserName.replace(/[,;=| ]+/g, \"_\")); &lt;/text&gt; &#125; appInsights.context.addTelemetryInitializer(function (envelope) &#123; if (envelope.name === Microsoft.ApplicationInsights.Telemetry.RemoteDependencyData.envelopeType &amp;&amp; envelope.data.baseData.target.indexOf(\"...\") &gt; -1) &#123; return false; &#125; if (envelope.name === Microsoft.ApplicationInsights.Telemetry.Exception.envelopeType &amp;&amp; envelope.data.baseData.message.contains(\"...\")) &#123; return false; &#125; &#125;); appInsights.viewPath = '&lt;nameofview&gt;'; appInsights.browserWidth = window.innerWidth; appInsights.browserHeight = window.innerHeight; appInsights.context.application.ver = '@typeof(&lt;Assembly.ClassName&gt;).Assembly.GetName().Version.ToString()'; appInsights.context.application.build = '@typeof(&lt;Assembly.ClassName&gt;).Assembly.GetName().Version.ToString()'; appInsights.trackPageView(null, null, &#123; urlReferrer: document.referrer &#125;);&#125;);The first bit is just the standard Application Insights javascript script. This you can find anywhere. Then we add our startup function to appInsights.queue.push. This way the startup function is called when application insights is fully loaded. Next, we set the user authentication context based on the current principal. You can find more on this in my blogpost “Follow a specific user with Application Insights by setting AuthenticatedUserId”. Next, we can add TelemetryInitializers, but we mostly use them to filter certain items. We have a javascript framework included in our app that we use to show popups or function tours to our users. We make calls to this system every request and I don’t have any need seeing these calls in our logs, so we filter those out. We also filter some exceptions that are caused by browser extensions. Finally, we set some values (which are pretty obvious I think) and we call trackPageView to log the page view with our values. This gives us lots of good information on what our users are doing and see all errors users experience with our javascript code.","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://eriksteinebach.com/tags/javascript/"},{"name":"application insights","slug":"application-insights","permalink":"http://eriksteinebach.com/tags/application-insights/"},{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":"tracking and tracing","slug":"tracking-and-tracing","permalink":"http://eriksteinebach.com/tags/tracking-and-tracing/"},{"name":".net","slug":"net","permalink":"http://eriksteinebach.com/tags/net/"},{"name":"user tracking","slug":"user-tracking","permalink":"http://eriksteinebach.com/tags/user-tracking/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"Generic client for sending events to Event Grid","slug":"generic-client-for-sending-events-to-eventgrid","date":"2018-05-14T13:05:45.000Z","updated":"2018-06-11T01:31:13.773Z","comments":true,"path":"2018/05/14/generic-client-for-sending-events-to-eventgrid/","link":"","permalink":"http://eriksteinebach.com/2018/05/14/generic-client-for-sending-events-to-eventgrid/","excerpt":"","text":"To support the use of Event Grid in our application architecture we decided to build a generic Event Grid client to send our events. This client enforces some practices like the use of base events and error logging. Event Grid event schemaEvent Grid events have a set schema which needs to be send and which Event Grid will again route to subscribers. The json looks like this (this is an event defined by Azure Storage): 12345678910111213141516171819202122232425[ &#123; \"topic\": \"/subscriptions/&#123;subscription-id&#125;/resourceGroups/Storage/providers/Microsoft.Storage/storageAccounts/xstoretestaccount\", \"subject\": \"/blobServices/default/containers/oc2d2817345i200097container/blobs/oc2d2817345i20002296blob\", \"eventType\": \"Microsoft.Storage.BlobCreated\", \"eventTime\": \"2017-06-26T18:41:00.9584103Z\", \"id\": \"831e1650-001e-001b-66ab-eeb76e069631\", \"data\": &#123; \"api\": \"PutBlockList\", \"clientRequestId\": \"6d79dbfb-0e37-4fc4-981f-442c9ca65760\", \"requestId\": \"831e1650-001e-001b-66ab-eeb76e000000\", \"eTag\": \"0x8D4BCC2E4835CD0\", \"contentType\": \"application/octet-stream\", \"contentLength\": 524288, \"blobType\": \"BlockBlob\", \"url\": \"https://oc2d2817345i60006.blob.core.windows.net/oc2d2817345i200097container/oc2d2817345i20002296blob\", \"sequencer\": \"00000000000004420000000000028963\", \"storageDiagnostics\": &#123; \"batchId\": \"b68529f3-68cd-4744-baa4-3c0498ec19f0\" &#125; &#125;, \"dataVersion\": \"\", \"metadataVersion\": \"1\" &#125;] Most fields are probably pretty logical, but I want to call out the data field, because this is where you can put your own “Event” data. To make it easy to work with I made a POCO class to represent the event schema. This way you can serialize back and forth easily between json and a class. The Data field is represented as a generic T, because this could be of any type. You could add a generic class or a JObject if you need a generic representation of the event (if you want to receive multiple different event types for example.1234567891011public class GridEvent&lt;T&gt; where T : class&#123; public string Id &#123; get; set; &#125; public string Subject &#123; get; set; &#125; public string EventType &#123; get; set; &#125; public T Data &#123; get; set; &#125; public DateTime EventTime &#123; get; set; &#125; public string Topic &#123; get; set; &#125; public string DataVersion &#123; get; set; &#125; public string MetadataVersion &#123; get; set; &#125;&#125;I made some base classes for specific events which would be in the Data block. For example every event in our organization contains a SourceApplicationName and we have the validationcode added in this event to support easier handling of returning the validationcode when a Event Grid event subscription is added. We have another base event that inherits from that one for user specific events with a UserId. All events need to inherit from these base events, something the generic client enforces.1234567891011public class AppGridEvent&#123; public string SourceApplicationName &#123; get; set; &#125; public string ValidationCode &#123; get; set; &#125; ...&#125;public class AppGridUserEvent : AppGridEvent&#123; public Guid UserId &#123; get; set; &#125; ...&#125; Generic clientSending events is easy. It is just a HTTP Post, but still there is value in having a generic client to do this. It can handle a couple of things for you that make life easier. ConfigurationOur clients constructor takes a couple of configuration values:TopicEndpointUrl: The url where events will be sendSasToken: The token used to authenticate with EventGrid (so you are allowed to send the event)ApplicationName: Name of the calling application so we always know the source (this is automatically set by the client)123456789101112131415161718192021222324252627282930313233public class EventClient : IEventClient&#123; private readonly EventClientConfig _config; private readonly TelemetryClient _telemetryClient; private readonly HttpClient _client; public EventClient(EventClientConfig config, TelemetryClient telemetryClient) &#123; if (config == null) throw new ArgumentNullException(\"config\"); if (config.TopicEndpointUrl == null) throw new ArgumentNullException(\"TopicEndpointUrl cannot be null\"); if (config.SasToken == null) throw new ArgumentNullException(\"SasToken cannot be null\"); if (config.ApplicationName == null) throw new ArgumentNullException(\"ApplicationName cannot be null\"); _config = config; _telemetryClient = telemetryClient; _client = new HttpClient(); client.DefaultRequestHeaders.Add(\"aeg-sas-key\", _config.SasToken); client.DefaultRequestHeaders.UserAgent.ParseAdd(_config.ApplicationName); &#125; public async Task&lt;bool&gt; SendEventAsync&lt;T&gt;(T eventData) where T : AppGridEvent &#123; ... &#125;&#125;These settings come from an object out of our appsettings.json, but obviously could come from where ever you like. In the constructor we also initialize the HttpClient. Note that every HttpClient constructor call registers a new port on your machine, so our EventClient needs to be used as a singleton or static in your application. 12345678910111213141516171819202122if (eventData == null) throw new ArgumentNullException(\"eventData\");var gridEvent = new GridEvent&lt;T&gt;()&#123; Id = Guid.NewGuid().ToString(), EventType = typeof(T).Name, EventTime = DateTime.UtcNow, Subject = eventData.Subject, Data = eventData, DataVersion = \"\"&#125;;gridEvent.Data.SourceApplicationName = _config.ApplicationName;string json = JsonConvert.SerializeObject(new List&lt;GridEvent&lt;T&gt;&gt;() &#123; gridEvent &#125;);HttpRequestMessage request = new HttpRequestMessage(HttpMethod.Post, _config.TopicEndpointUrl)&#123; Content = new StringContent(json, Encoding.UTF8, \"application/json\")&#125;;var result = await _client.SendAsync(request); The client receives a generic object of base type AppGridEvent which contains the base fields like ApplicationName. This base type is serialized in json and a HttpRequestMessage is created. This message is send to EventGrid and we record if a successful message is return. One open point right now is versioning. When the time comes when we need it, I plan to get the version from an attribute on the POCO class. Catching exceptionsIn our application events that we throw don’t need to be transaction. We want as many as possible, but if we sometimes miss some, that is fine. So we catch any errors that occur on sending of the event, because we don’t want this to bubble up to the user. This works very well for us, because we rather miss an event than the user being impacted by one of these errors. But keep in mind, this might not work for everyone/all type of events.1234567891011121314try&#123; ... if (!result.IsSuccessStatusCode) &#123; _telemetryClient.TrackException(new Exception($\"SendEventAsync resulted in invalid statuscode &#123;result.StatusCode.ToString()&#125;\")); &#125; return result.IsSuccessStatusCode;&#125;catch (Exception exc)&#123; _telemetryClient.TrackException(exc); return false;&#125;In case of an incorrect result status code we log this code, but you could also throw an exception here. Keep in mind that you will need to check this, just catching exceptions is not enough to see all failed requests. Sharing of events (event types)At the moment we use a nuget package to share events definitions (in the form of the POCO classes) between the different applications that use this event. This package is stored in a private Visual Studio online package repositry. This works very well and enforces that when the event is changed (and the nuget package is updated) the applications break. But it does somewhat break the principle that the source application is the owner of the schema. Until now this approach has been working well for us. But I am always open for suggestions, so if you see possible improvements, please let me know!","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":".net","slug":"net","permalink":"http://eriksteinebach.com/tags/net/"},{"name":"event grid","slug":"event-grid","permalink":"http://eriksteinebach.com/tags/event-grid/"},{"name":"eventing","slug":"eventing","permalink":"http://eriksteinebach.com/tags/eventing/"},{"name":"microservices","slug":"microservices","permalink":"http://eriksteinebach.com/tags/microservices/"},{"name":"http","slug":"http","permalink":"http://eriksteinebach.com/tags/http/"},{"name":"client","slug":"client","permalink":"http://eriksteinebach.com/tags/client/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"Azure Event Grid vs Azure Service Bus","slug":"azure-eventgrid-vs-azure-servicebus","date":"2018-05-10T22:56:16.000Z","updated":"2018-05-17T23:57:39.327Z","comments":true,"path":"2018/05/10/azure-eventgrid-vs-azure-servicebus/","link":"","permalink":"http://eriksteinebach.com/2018/05/10/azure-eventgrid-vs-azure-servicebus/","excerpt":"","text":"When I started looking into solutions for routing events to our new microservice, it took me a while to get a clear picture of the differences between Azure Event Grid and Azure Service Bus. In this blogpost I want to share my findings and try to show what the differences are. Hopefully this will help you find the best solution for your problem, because these are not a one or the other products. Both Event Grid and Service Bus have different characteristics, and both can be a valid solution for your problem. Azure Event Grid Event Grid is more like a router for events Push based model, based on HTTP calls (both ingoing and outgoing) Event Grid has an available endpoint where you can send events (http post). When receiving events, EventGrid will route them to subscribers, based on the configuration made in Event Grid (This can be done through the portal or with a management API) With pay on use solutions (with for example Azure functions) you don’t pay anything when there is no traffic. Not for Event Grid or for your receiving and sending application, because there are no polling threads that need to be active) Retry mechanism Retries at 10, 30 seconds, 1, 5, 10, 30 minutes, 1 hour, every hour up till 24 hours, with small randomization of those times Filtering Can be done by EventType and Subject (custom string per event) For Subject only filtering on StartsWith and EndsWith are available (nothing more complex) Not transactional (because of retries you have a reasonable guarantee of at least once delivery) Doesn’t have a local emulator, so local testing is tricky Azure Service Bus Service Bus is a queue-based system Pull based, receiving needs to pull a queue (ingoing is writing to queue, outgoing is pull from queue) The application writes an event to a queue, the receiving system monitors this queue. With Azure Functions/WebJob polling the queue is fully abstracted, but there will always be an active thread Retry mechanism Can be customized by reader, but both writing, and reading could be done 100% transactional Azure functions/webjob triggers offer an out of the box solution for this Filering Much more customization possibilities compared to Event Grid. SQL style language to write filters for topics 100% transactional (in and out) Service Bus also doesn’t have a good local emulator, but I could imagine using an azure queue as an alternative should be possible. I didn’t test this or do much research on this though For my project Event Grid was the system we were looking for. We are building a notification system, where 100% accurate delivery is not required. The cost savings and ease of development were the deciding factors for us to go with Event Grid. We almost switched to Service Bus when we figured out that there was no local emulator, because this is quite limiting during development, but because Service Bus has the same problem, we went with Event Bus anyway. I hope this comparison will help you pick the correct eventing solution for you.","categories":[{"name":"Architecture","slug":"Architecture","permalink":"http://eriksteinebach.com/categories/Architecture/"}],"tags":[{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":".net","slug":"net","permalink":"http://eriksteinebach.com/tags/net/"},{"name":"event grid","slug":"event-grid","permalink":"http://eriksteinebach.com/tags/event-grid/"},{"name":"service bus","slug":"service-bus","permalink":"http://eriksteinebach.com/tags/service-bus/"},{"name":"eventing","slug":"eventing","permalink":"http://eriksteinebach.com/tags/eventing/"},{"name":"microservices","slug":"microservices","permalink":"http://eriksteinebach.com/tags/microservices/"},{"name":"architecture","slug":"architecture","permalink":"http://eriksteinebach.com/tags/architecture/"}],"keywords":[{"name":"Architecture","slug":"Architecture","permalink":"http://eriksteinebach.com/categories/Architecture/"}]},{"title":"Follow a specific user in Application Insights .NET Core edition","slug":"specific-user-application-insights-netcore","date":"2018-05-06T17:20:03.000Z","updated":"2018-05-17T23:57:02.422Z","comments":true,"path":"2018/05/06/specific-user-application-insights-netcore/","link":"","permalink":"http://eriksteinebach.com/2018/05/06/specific-user-application-insights-netcore/","excerpt":"","text":"In 2016 I wrote how to track a specific user in Application Insights which has been very useful in tracking down production issues. Now we are starting to upgrade our project to .NET Core so I wanted to share how to do this in .NET Core. In .NET Core you can use the same method, the ITelemetryInitializer: 123456789101112131415161718public class AuthenticatedUserInitializer : ITelemetryInitializer&#123; private IHttpContextAccessor _httpContextAccessor; public AuthenticatedUserInitializer(IHttpContextAccessor httpContextAccessor) &#123; _httpContextAccessor = httpContextAccessor ?? throw new ArgumentNullException(\"httpContextAccessor\"); &#125; public void Initialize(ITelemetry telemetry) &#123; var httpContext = _httpContextAccessor.HttpContext; if (httpContext?.User?.Identity?.Name != null &amp;&amp; httpContext.User.Identity.IsAuthenticated == true) &#123; telemetry.Context.User.AuthenticatedUserId = httpContext.User.Identity.Name; telemetry.Context.User.AccountId = httpContext.User.FindFirst(\"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier\")?.Value ?? telemetry.Context.User.AccountId; &#125; &#125;&#125; The interface is the same. To get access to the users information we need to access the user in the HttpContext. In .NET Core you get access to the HttpContext through the “IHttpContextAccessor” interface. Depending on the setup of your identity you can select the required information. We use our users email address (which is accessed through Identity.Name) as the AuthenticatedUserId. For the AccountId we use the unique user id (Guid in our case). In case no userId is available we use the accountId generated by Application Insights.","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"asp.net","slug":"asp-net","permalink":"http://eriksteinebach.com/tags/asp-net/"},{"name":"application insights","slug":"application-insights","permalink":"http://eriksteinebach.com/tags/application-insights/"},{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":"tracking and tracing","slug":"tracking-and-tracing","permalink":"http://eriksteinebach.com/tags/tracking-and-tracing/"},{"name":"user tracking","slug":"user-tracking","permalink":"http://eriksteinebach.com/tags/user-tracking/"},{"name":".net core","slug":"net-core","permalink":"http://eriksteinebach.com/tags/net-core/"},{"name":"netcore","slug":"netcore","permalink":"http://eriksteinebach.com/tags/netcore/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"Follow a specific user with Application Insights by setting AuthenticatedUserId","slug":"Follow-a-specific-user-with-Application-Insights-by-setting-AuthenticatedUserId","date":"2016-12-26T14:32:08.000Z","updated":"2018-05-17T23:57:37.418Z","comments":true,"path":"2016/12/26/Follow-a-specific-user-with-Application-Insights-by-setting-AuthenticatedUserId/","link":"","permalink":"http://eriksteinebach.com/2016/12/26/Follow-a-specific-user-with-Application-Insights-by-setting-AuthenticatedUserId/","excerpt":"","text":"In my latest project we embraced Application Insights as our logging and tracking platform. I am impressed by its capabilities and ease of use when hosting on Azure. It already has been very useful and it is free, because the amount of data we track is within 1 GB or 5M data points per month. User trackingThe only thing I felt was missing was (specific) user tracking. Out of the box AppInsights stores a user key with every tracked item, but this is a random value. This is good, because it lets you track and group all the telemetry of one user, but we don’t know who that user is. We wanted to add a username or email to the telemetry so we could track a specific user. Why we wanted to track a specific userThere is for me no doubt Application Insights is very useful, only last week I fixed 3 exceptions before the user’s could let us know things were broken, because they showed up in App Insights. Also this week I noticed a javascript error in a browser version we don’t test ourselves because of time constrains. An error we otherwise would have missed. So both clear wins! A good example, why tracking the user is useful, was a user who said there was a bug in the system. We couldn’t reproduce the issue, but by looking at his session we figured out he was using the system differently then we anticipated. It did spark a discussion if the system was clear enough, but it turned out not to be a bug. A good thing to know and it was very useful and a time saver that we could track the user. Connecting your user system with Application InsightsApplication Insights out of the box generates a random value because it doesn’t know about your users. You might be using Active directory, ASP.NET Identity, a custom solution or something else, so you have to link those up yourself. App Insights can track server and client side. This means it tracks items on the server within ASP.NET and it tracks client side in the javascript running in the users browsers. The server side tracks for example Requests, Dependencies and Exceptions. On the client it tracks for example page views and user events like a button click. On all those request we wanted to add the username. How to track the user server sideImplement the ITelementryInitializer interface. The Initialize method is called on every telemetry item that is tracked. So it is important to keep the method very simple to not slow down your application. It will be called a lot. The easiest, and what we ended up doing, is set the CurrentPrincipal Identity Name as the AuthenticatedUserId. AuthenticatedUserId is a standard property available on every tracked item and in our system the Identity.Name is set to the users username (which is their email address). This is what the class looks like: 123456789101112public class AppInsightsInitializer : ITelemetryInitializer&#123; public void Initialize(ITelemetry telemetry) &#123; if (Thread.CurrentPrincipal != null &amp;&amp; Thread.CurrentPrincipal.Identity != null) &#123; telemetry.Context.User.AuthenticatedUserId = Thread.CurrentPrincipal.Identity.Name; &#125; &#125;&#125;When you created this Initializer you need to load it on application load. You can do this by adding this to you Startup.cs or Global.asax:12TelemetryConfiguration.Active.TelemetryInitializers .Add(new AppInsightsInitializer()); How to track the user on the clientFor client side Application Insights tracking we already added some javascript to every page. We set the user by calling the setAuthenticatedUserContext on the appInsights object. This method sets the same AuthenticatedUserId. Here we also take the Identity Name as the value:1234var appInsightsUserName = '@User.Identity.Name.Replace(\"\\\\\", \"\\\\\\\\\")';appInsights.setAuthenticatedUserContext( appInsightsUserName.replace(/[,;=| ]+/g, \"_\"));appInsights.trackPageView();Now all your Application Insights telemetry contains the username of the user (when that user is logged in at least). Happy user tracking! More information can be found here:https://docs.microsoft.com/en-us/azure/application-insights/app-insights-api-custom-events-metrics Edit: Updated version for .NET Core","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://eriksteinebach.com/tags/javascript/"},{"name":"asp.net","slug":"asp-net","permalink":"http://eriksteinebach.com/tags/asp-net/"},{"name":"application insights","slug":"application-insights","permalink":"http://eriksteinebach.com/tags/application-insights/"},{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":"tracking and tracing","slug":"tracking-and-tracing","permalink":"http://eriksteinebach.com/tags/tracking-and-tracing/"},{"name":".net","slug":"net","permalink":"http://eriksteinebach.com/tags/net/"},{"name":"user tracking","slug":"user-tracking","permalink":"http://eriksteinebach.com/tags/user-tracking/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"Issues continuous deploying a ASP.NET core 1.0 / AngularJS 2.0 app into Azure","slug":"issues-continuous-deploying-a-aspnetcore-1-angularjs-2-app-into-azure","date":"2016-07-29T21:25:00.000Z","updated":"2018-05-17T23:56:43.573Z","comments":true,"path":"2016/07/29/issues-continuous-deploying-a-aspnetcore-1-angularjs-2-app-into-azure/","link":"","permalink":"http://eriksteinebach.com/2016/07/29/issues-continuous-deploying-a-aspnetcore-1-angularjs-2-app-into-azure/","excerpt":"","text":"For a personal project (and to work with some new technology) I build a new ASPNET Core 1.0 website using AngularJS2. To make further development easier for myself, I wanted to enable continuous deployment, so my web application would be deployed into Azure on every checkin. Because the used technology is all relatively new, I didn’t expect it to work the first try and it took me some time to get it to work correctly. I included my errors and solutions for anyone who is doing the same and runs in the same issues. I expect most will come here by googling one of the errors ;). My approachI used Yeoman’s template generator-aspnetcore-spa to generate the basis of this project, you can find the how to here.1npm install -g yo generator-aspnetcore-spaThen I used this tutorial to deploy this aspnetcore app to azure. A quick note here, I used VSTS deployment, so the deployment is configured in VSTS, so I am not using Kudu (this makes a difference, more on this later). I hoped the deployment would be smooth, but not quite. I will discribe the issues and explain how I solved them. Issue number 1: Web Deploy cannot modify the file ‘Client.dll’ (Using the “Deploy Website to Azure” task)[error]Microsoft.Web.Deployment.DeploymentDetailedClientServerException: Web Deploy cannot modify the file ‘Client.dll’ on the destination because it is locked by an external process. In order to allow the publish operation to succeed, you may need to either restart your application to release the lock, or use the AppOffline rule handler for .Net applications on your next publish attempt. Learn more at: http://go.microsoft.com/fwlink/?LinkId=221672#ERROR_FILE_IN_USE. This error is caused by the application still being active when you are deploying. The task doesn’t stop the application, which you also not always want to happen. The “old” “Deploy Website to Azure” release task works, but doesn’t include the option to take the app offline before deploying. Initially I switched to the “Deploy AzureRM Web App” release task, because this task does support that option. But because of another issue (see the next error) I decided the switch back and I solved this problem by shutting the app down, before the deploy and starting it back up after. This is no problem for my app, but this might not be ideal for every website. You can add powershell steps, so you can do whatever works in your scenario. Issue number 2: Error: Source does not support parameter called ‘IIS Web Application Name’ with a green Build (Using the “Deploy AzureRM Web App” task)The “Deploy AzureRM Web App” runs msdeploy under the hood to deploy to azure. I ran into 2 issues here: I got the error: “Error: Source does not support parameter called ‘IIS Web Application Name’. Must be one of ().” and given that error, the build was green. The green build is a known issue, see here. So this should be solved soon. The other issue was trickier, I found two solutions: http://www.factus.dk/post/2016/07/04/Build-and-deployment-of-ASPNET-Core-10-to-Azure-App-Service https://github.com/Microsoft/vsts-rm-extensions/issues/51#event-698986554 Based on this information I added a parameters.xml and set.parameters.xml file to my project and configured the build to use these files. This solved this problem, but then gave me the following error: Issue number 3: Error Code: ERROR_INSUFFICIENT_ACCESS_TO_SITE_FOLDERMore Information: Unable to perform the operation (“Create Directory”) for the specified directory (“C:\\a\\1\\s...\\src\\Client\\bin...\\”). This can occur if the server administrator has not authorized this operation for the user credentials you are using. Learn more at: http://go.microsoft.com/fwlink/?LinkId=221672#ERROR_INSUFFICIENT_ACCESS_TO_SITE_FOLDER.Error: The error code was 0x80070005.Error: Access to the path ‘C:\\a’ is denied. It looks like the contentPath was not being set correctly in the msdeploy package, so I tried setting it myself through the parameter file and through commandline arguments, but in both situations without result. Because I was not sure what was happening I decided to switch back to the “Deploy Website to Azure” task, because that one worked fine. I just had to add the powershell scripts to stop and start the website. Edit: After I finished writing this blogpost I got a possible solution to this problem on Github. After the weekend I will try this out and see if this solves my error. Issue number 4: Exception: Call to Node module failed with error: To use prerendering, you must install the ‘aspnet-prerendering’ NPM package.So now my application was being deployed through continuous delivery, but my website was not working yet. Locally my application ran fine, but on Azure I got the following exception:Exception: Call to Node module failed with error: To use prerendering, you must install the ‘aspnet-prerendering’ NPM package. I temporarily changed:12&lt;app asp-prerender-module=\"ClientApp/boot-server\" asp-prerender-webpack-config=\"webpack.config.js\"&gt;Loading...&lt;/app&gt;into:1&lt;app&gt;&lt;/app&gt;That did the trick, but this would lose me the prerendering which is actually pretty cool and useful. So what is going wrong here? It sounds like this is a problem not related to deployment, but it turns out it actually is. The cause of this error is that the folder “node_modules” is not being deployed, and aspnetcore needs those for the server side node execution. This is because in the generator-aspnetcore-spa template a .gitignore file is added that blocks (ignores) the “node_modules” folder from being checked in. The idea behind this (I assume) is that during deployment the correct modules are downloaded and added to the website so it all works again. This has some advantages and disadvantages. I am not sure what the best way to go is, so I think I will write a blogpost about this to order my own thoughts on this. If you have ideas about this let me know in the comments. And this also brought up another point which I didn’t realize: There is a difference between deploying from VSTS into Azure and attaching your source repository in VSTS (or other git host) to your Azure website. Deployment from VSTS is with MSBuild, deployment from Azure goes via Kudu. And deploying the same solution over MSBuild or via Kudu gives a different result. This is something to be aware of. I think I will write another blogpost with my feelings about Kudu. For the details about this issue see my github question. .article-entry li{ margin-left: 2.2em; list-style-position: outside; }","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}],"tags":[{"name":"asp.net","slug":"asp-net","permalink":"http://eriksteinebach.com/tags/asp-net/"},{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":"devops","slug":"devops","permalink":"http://eriksteinebach.com/tags/devops/"},{"name":"dotnetcore","slug":"dotnetcore","permalink":"http://eriksteinebach.com/tags/dotnetcore/"},{"name":"angularjS","slug":"angularjS","permalink":"http://eriksteinebach.com/tags/angularjS/"},{"name":"vsts","slug":"vsts","permalink":"http://eriksteinebach.com/tags/vsts/"},{"name":"yeoman","slug":"yeoman","permalink":"http://eriksteinebach.com/tags/yeoman/"},{"name":"kudu","slug":"kudu","permalink":"http://eriksteinebach.com/tags/kudu/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}]},{"title":"Creating charts with d3.js and ASP.NET MVC","slug":"Creating-charts-with-d3js-and-ASPNET-MVC","date":"2016-05-25T13:24:47.000Z","updated":"2018-05-17T23:55:34.473Z","comments":true,"path":"2016/05/25/Creating-charts-with-d3js-and-ASPNET-MVC/","link":"","permalink":"http://eriksteinebach.com/2016/05/25/Creating-charts-with-d3js-and-ASPNET-MVC/","excerpt":"","text":"For a recent project I needed to create a line chart and a bar chart. Previously I used the ASP.NET chart controls, but in the new ASP.NET Core the chart control is not available anymore, so I went looking for something new. Because of the many single page applications build in javascript these days I figured a javascript based solution would be the best investment for the future. With client computers and browsers being fast enough, there really is no need anymore to generate charts on the server. After some research I choose D3.js. “D3.js is a javascript library for producing dynamic, interactive data visualizations in web browsers. It makes use of the widely implemented SVG, HTML5, and CSS standards. It is the successor to the earlier Protovis framework.” - Wikipedia D3.js is a highly flexible and powerful library to build a variety of different data visualizations. But the downside of being so powerful is that it is not so easy to understand at first. That is why I wanted to make this blog post. I am going to assume you are a developer like me, so you have knowledge of HTML, CSS, Javascript and for the .NET parts also knowledge of ASP.NET. I will walk you through creating a bar chart and a line chart. And will explain how to get the data for your chart from the server. After that you should have a basic understanding of D3.js, to also be able to build other visualizations. See other visualizations here. How to make a bar chartTo build a chart we start with a basic page, with the d3.js reference and a SVG (Scalable Vector Graphics) element. This SVG is used to draw the chart. With javascript we give the SVG a width and height and we store those in a variable, because we will need those later. We also have a set of data that we can use to draw. Later we will take the data from a JSON call.123456789101112&lt;script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"&gt;&lt;svg id=\"barChart\"&gt;&lt;/svg&gt;&lt;script&gt;var data = [ 50,90,30,10,70,20];var w = 500;var h = 100;var svg = d3.select(\"#barChart\") .attr(\"width\", w) .attr(\"height\", h);&lt;/script&gt;That is the basic setup, we continue with drawing the bars:123svg.selectAll(\"rect\") .data(data) .enter()This takes the data and for every item in the array (basic number or complex type) makes a rectangle. After the enter() function every other chained function is called for every rectangle. Because every rectangle needs an x and y (start position coordinate) and a width and height we will add those like this:1234.attr(\"x\", 0).attr(\"y\", 0).attr(\"width\", 20).attr(\"height\", 100);But this only gives us what looks like 1 rectangle (it are actually 6 rectangles on top of each other), because the values obviously are bar specific. First we change the x for every rectangle:123.attr(\"x\", function(d, i) &#123; return i * 21; //Bar width of 20 plus 1 for padding&#125;)This moves every bar next to the previous bar (the bars are 20 pixels wide, plus 1 pixel padding inbetween). Then we change the height so it has the height of the value:123 .attr(\"height\", function(d) &#123; return d; //Just the data value&#125;);But now the bar is upside down, so we correct the y to start so all the bars line out on the bottom:123.attr(\"y\", function(d) &#123; return h - d; //Height minus data value &#125;)Here is what we have so far, you can play with the javascript to see what happens with the bars:JS Bin on jsbin.comI added some color to the chart with the fill attribute, just to make it a little nicer. You can use javascript or CSS to style your chart (later you will see a css example). The example is nice, but this only works because the data matches the pixels of the chart. What if they don’t, because that is like.. always? Well then you use scales, a functions to map from an input domain to an output range. To be able to show a full bar chart we make the data a bit more complex:123456var data = [ &#123; Product: \"Shoes\", Count: 5 &#125;, &#123; Product: \"Shirts\", Count: 9 &#125;, &#123; Product: \"Pants\", Count: 3 &#125;, &#123; Product: \"Ties\", Count: 1 &#125;, &#123; Product: \"Socks\", Count: 7 &#125;, &#123; Product: \"Jackets\", Count: 2 &#125;];Probably a more realistic dataset. So we make scales, an ordinal scale (more info here) for the bars, and a linear scale for the height of the bars:1234567//An ordinal scale, to support the bars, we choose var x = d3.scale.ordinal() .rangeRoundBands([width, 0], 0.1); //I think this makes a lot of sense (just a default scale converter)var y = d3.scale.linear() .range([0, height]); And we add the “mapping”, the domain:1234//The x domain is a map of all the Products namesx.domain(data.map(function(d) &#123; return d.Product; &#125;));//The y domain is a range from the maximal (Count) value in the array until 0y.domain([d3.max(data, function(d) &#123; return d.Count; &#125;), 0]);Then we can use that information to set the right x,y,width,height info for the bars. You use x() and y() functions for that. The rangeBand function is to help set the width for the bars:123456789101112.attr(\"x\", function(d) &#123; //the x function, transforms the value, based on the scale return x(d.Product); &#125;)//The rangeBand() function returns the width of the bars.attr(\"width\", x.rangeBand()) .attr(\"y\", function(d) &#123; return y(d.Count); //the y function does the same&#125;).attr(\"height\", function(d) &#123; return height - y(d.Count);&#125;);Now you have a fully working bar chart (with correct scaling):JS Bin on jsbin.comThis is a nice start, but I think most people would like to add some axis (see the inline comments for more info):123456789101112131415161718192021222324var xAxis = d3.svg.axis() //Create an axis .scale(x) //scale the axis .orient(\"bottom\"); //this is where the labels will be locatedvar yAxis = d3.svg.axis() .scale(y) .orient(\"left\") .tickFormat(d3.format(\"d\")) //Ticks are the divisions on the scale. .tickSubdivide(0); //Here we want to see only whole numbers on the axissvg.append(\"g\") .attr(\"class\", \"y axis\") .call(yAxis);svg.append(\"g\") .attr(\"class\", \"y axis\") .attr(\"transform\", \"translate(0,\" + height + \")\") .call(xAxis) .selectAll(\"text\") //In some cases the labels will overlap .attr(\"transform\", \"rotate(90)\") //so you might for example want to rotate the label's so they are vertical .attr(\"x\", 0) //After that you might have to do some finetuning to align everything the right way: .attr(\"y\", -6) .attr(\"dx\", \".6em\") .style(\"text-anchor\", \"start\");And now we have a full chart (I also added some CSS styling):JS Bin on jsbin.comOr as an extra example, the same data in a bar chart with horizontal bars. I wanted to share this example as well, because it turned out to be a bit more complex then I expected. Comparing them is a good way to see if you understand D3.js How to make a line chartAnother very common chart is the line chart. I build on top of what we learned with the bar chart, so I assume you understand the scaling:1234567891011121314151617181920212223242526272829303132333435363738 &lt;svg id=\"lineChart\"&gt;&lt;/svg&gt; &lt;style&gt; .line &#123; fill: none; stroke: steelblue; stroke-width: 2px; &#125; &lt;/style&gt;&lt;script&gt; var data = [50,90,30,10,70,20]; var width = 500; var height = 100; var svg = d3.select(\"#lineChart\") .attr(\"width\", width) .attr(\"height\", height); var x = d3.scale.linear() .range([0, width]); var y = d3.scale.linear() .range([height, 0]); x.domain([0, data.length]); y.domain([0, d3.max(data, function (d) &#123; return d; &#125;)]); var line = d3.svg.line() .x(function (d, i) &#123; return x(i); &#125;) .y(function (d) &#123; return y(d); &#125;); svg.append(\"path\") .datum(data) .attr(\"class\", \"line\") .attr(\"d\", line); &lt;/script&gt;This results in this line chart:JS Bin on jsbin.comVery basic, but a line chart. Some things of note for the chart example: The scales are both linear The domains are set to size of the array and to the max value in the array The line is drawn by plotting the index of the array against the value in the array Then we add the line to the svg, as a path If you do not add any style for the line, it will look a bit strange, because it will fill the object by default (just try it out to see, remove the style block in the JSBin example), so normally you will add a style. Of course you can also add axis to this if you want, in the same way as with the bar chart. Now the only thing left is styling your charts to make them look great! Get data via an JSON call (ASP.NET MVC)Mmost of the time the information for the chart comes from a database on the server. Although you could generate the javascript to create the data array, a cleaner solution is to make an JSON call to fetch the data. D3.js includes a perfect function to do this:12345 d3.json(\"MyController/MyAction\", function (data) &#123; //Here you have data available, an object with the same structure //as the JSON that was send by the server.&#125;);In ASP.NET Core 1.0 the function that will be called should look like this:1234567891011public ActionResult MyAction()&#123; return Json( new[] &#123; new &#123; Product = \"Shoes\" , Count = 5 &#125;, new &#123; Product = \"Shirts\" , Count = 9 &#125;, new &#123; Product = \"Pants\" , Count = 3 &#125;, new &#123; Product = \"Ties\" , Count = 1 &#125;, new &#123; Product = \"Socks\" , Count = 7 &#125;, new &#123; Product = \"Jackets\" , Count = 2 &#125; &#125;);&#125;And with that you have a complete chart with data from the server. If you need an “almost no development skills”, or a more indepth tutorial for D3.js you can also look here. Let me know if you have any questions. Happy charting! .article-entry li{ margin-left: 2.2em; list-style-position: outside; }","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"http://eriksteinebach.com/tags/javascript/"},{"name":"d3.js","slug":"d3-js","permalink":"http://eriksteinebach.com/tags/d3-js/"},{"name":"charts","slug":"charts","permalink":"http://eriksteinebach.com/tags/charts/"},{"name":"graphs","slug":"graphs","permalink":"http://eriksteinebach.com/tags/graphs/"},{"name":"linechart","slug":"linechart","permalink":"http://eriksteinebach.com/tags/linechart/"},{"name":"barchart","slug":"barchart","permalink":"http://eriksteinebach.com/tags/barchart/"},{"name":"asp.net","slug":"asp-net","permalink":"http://eriksteinebach.com/tags/asp-net/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"How to deal with time (in .NET and on Azure)","slug":"how-to-deal-with-time","date":"2016-05-17T21:28:43.000Z","updated":"2018-05-17T23:56:02.114Z","comments":true,"path":"2016/05/17/how-to-deal-with-time/","link":"","permalink":"http://eriksteinebach.com/2016/05/17/how-to-deal-with-time/","excerpt":"","text":"Handling time in .NET applications, it seems straight forward (“just use the .NET DateTime object”) and in some scenario’s it also is, but not for everything. In your local hobby project you will be fine, but I am sure everyone venturing further than that, will have experienced that working with time in .NET is not always so easy and/or logical. I will cover 3 different scenarios that I have encountered and how we solved those. This is by no means a complete list, but I do think they cover the main scenarios: A locally developed website used by people in the same country and hosted in that country A relatively small application, used by people in the same country, but hosted on Azure An application with user from all over the world 1. A locally developed website used by people in the same country and hosted in that countryThis scenario is why I think not everyone runs into trouble with DateTime when they first start out developing .NET applications. If we make the assumption that the development machine, the user machines and the server (host) machine are all configured with the same time zone, the .NET DateTime object works fine. You don’t really need time conversion in your application, which makes the application definitely less complex. Yay for you! There are some things to consider though: Does your time zone have summer and winter time? If so what happens in that hour that overlaps each year? Will this cause a problem in your application? It might for example mix up events occurring in the hour before the switch and the hour after. Is this a problem? If not, don’t worry to much about it, I am a big proponent of lean development. Leave it be for now. But a possible solution in this situation would be to store time in UTC time. This makes it a little bit more complex, but will solve those problems. You can use the build-in ToUniversalTime and ToLocalTime on the DateTime object to do this. What happens when one of your users wants to use the application when he goes to another time zone? This is not a problem if you can tell your user he has to input the time based on your applications time zone, but of course that is not very user friendly, so it is something to consider. To solve this your application will have to implement scenario 3. Your product owner will have to decide if that investment is worth the money. 2. A relatively small app, used by people in the same country, but hosted on AzureSo this scenario is why I ran into trouble with DateTime. My application is for users in Peru, so I only wanted to convert to the Peruvian time zone from UTC and the other way around. Scenario 3 was overkill in this situation, so I was looking for a simpler solution. To make it confusing, my development machine was set in local Peruvian time and the server the application was hosted on was in UTC (Azure). So the application responded differently on my development machine compared to Azure. Of course differences between development and productions are unwanted, but I think in this case for most people unavoidable. Who sets his development machine in UTC time (let me know!)? Some people maybe, but everyone who also does normal work on there computer can’t. Another case for automated testing! So how can you handle these situations?If you try to solve this problem with the standard DateTime object you run into a problem. For example, the user needs to select a date or time, so you make a select field (with a nice datetimepicker) and store the datetime in a DateTime object in your MVC model. On the server you immediately convert the datetime to UTC. Sounds easy, but wait… My first instinct was to do something this (like scenario 1):Basic (but wrong) conversion123//This is the same thing that happens when the MVC model is filledvar date = DateTime.Parse( \"2016-5-16 18:59:00\"); var utc = date.ToUniversalTime();And this works on my development machine, but fails on Azure. DateTime interprets the datetime based on the computer time zone, because the Kind property is set to Unspecified (during model binding). So on my development PC it uses a 5 hour UTC offset (based on my computers time zone), but on Azure the UTC offset is 0 (because Azure servers run on UTC time). So where my local machines stored the right UTC time in the database, on Azure the time the user inputted was directly stored as UTC in the database. To solve this you have to use a specific time zone to convert the value:Convert time with timezone1234//This is the same thing that happens when the MVC model is filledvar date = DateTime.Parse( \"2016-5-16 18:59:00\"); TimeZoneInfo tzi = TimeZoneInfo.FindSystemTimeZoneById(\"SA Pacific Standard Time\");var utc = TimeZoneInfo.ConvertTimeToUtc(date , tzi); This means you need to know the time zone of the user. If you have an application where only one time zone is required this can be a config setting. 3. An application with users from all over the worldLet’s say you are building twitter. Every user of twitter wants to see time in his time zone, and even if he goes overseas he should see the correct time. There are 2 ways to approach this problem: Always use the time from the browser the user is currently browsing from (via javascript) to show the correct time. Save the user time zone in a user profile In both cases you want to store time in UTC so wherever the user is from, events are always ordered in the right way. The first approach works very well when you have a javascript client, like an AngularJS website, because browser time will always be available. All your server code will use UTC time and on the client you can convert to the browser time zone. This has an advantage that it also corrects the time when a user is traveling (assuming he/she changes the computer/cellphone time of course). When doing anything with time, you make your own life so much easier when using momentJS (http://momentjs.com/). With momentJs you can do the conversion like this:Conversion with momentJS1234var timeZoneName = \"America/Lima\";var originalUtcDate = moment.utc();var convertedDate = originalUtcDate.tz(timeZoneName);var utcDate = convertedDate.utc();When you use ASP.NET MVC for example (or another traditional website) it is harder to get access to the browser time. So here it is easier to store the time zone of the user in his/her profile so you can access it on the server. In this case the user will have to change the time zone on the website when he/she travels. When you know the timezone the user is in, you can convert the time from and to UTC in C# like this:Convert time with timezone1234TimeZoneInfo tzi = TimeZoneInfo.FindSystemTimeZoneById(\"SA Pacific Standard Time\");var originalUtcDate = DateTime.UtcNow;var convertedDate = TimeZoneInfo.ConvertTimeFromUtc(originalUtcDate, tzi);var utcDate = TimeZoneInfo.ConvertTimeToUtc(convertedDate, tzi);Another option, which I haven’t tried myself, but the idea sounds very interesting. You can still only use UTC time in MVC, and use javascript to convert times in the browser to the correct local time. This means identifying all datetime fields on page load and replace the values with corrected values and hook into form post events to change the values on post as well. It would be possible to write generic functions to do this, which would make life a lot easier and less prone for mistakes. SummaryTLTR, some best practices for working with DateTime in .NET: Always store datetime information in the database in UTC Know which time zone the user is working in. Make it explicit in some way (read the articles for options). When working with javascript apps (for example AnjularJS), use the browser time and work on the server and on the wire always in UTC. This is a great clear boundary. In MVC websites, this is harder, but I would advise converting to UTC as fast as you can. A good option is using client side javascript for this (because it makes it possible to use the browser time to convert to UTC) otherwise as soon as you are on the server (for example only use user time in the Model). Understand that time conversion is super tricky, definitely when dealing with different time zones. Use official tested frameworks to do this. You can use momentJS (http://momentjs.com/) for javascript and/or Noda Time (http://nodatime.org/) in C# if you do any kind of complex datetime conversion math. If you are interested in more best practices, see this comprehensive guide. This is not a complete guide, but a view in the complexities of time zone programming. If you have anything to add, do not agree or have other scenario’s to add, please leave a comment. .article-entry li{ margin-left: 2.2em; list-style-position: outside; }","categories":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}],"tags":[{"name":"azure","slug":"azure","permalink":"http://eriksteinebach.com/tags/azure/"},{"name":".net","slug":"net","permalink":"http://eriksteinebach.com/tags/net/"},{"name":"dateTime","slug":"dateTime","permalink":"http://eriksteinebach.com/tags/dateTime/"},{"name":"time","slug":"time","permalink":"http://eriksteinebach.com/tags/time/"}],"keywords":[{"name":"Coding","slug":"Coding","permalink":"http://eriksteinebach.com/categories/Coding/"}]},{"title":"Blog reboot and changes","slug":"blog-reboot-and-changes","date":"2016-05-15T00:36:00.000Z","updated":"2018-05-17T23:55:17.527Z","comments":true,"path":"2016/05/14/blog-reboot-and-changes/","link":"","permalink":"http://eriksteinebach.com/2016/05/14/blog-reboot-and-changes/","excerpt":"","text":"A new blogpost! Maybe you do not remember, but you subscribed to my blog. Don’t worry, it is totally possible you do not remember, because my last post was almost 2 years ago. I was hoping to be able to blog while I was traveling, but it turned out I was to busy with discovering the world (for more info see http://eriksteinebach.nl to do so. Now that I settled back down in Lima (Peru) for the time being, I want to reboot this blog. This means there will be new blogpost, a new design and that means a new blog system. The new content will be a bit more broad then before. Until now I focused on DevOps, but I am originally a .NET developer and I am going back to my roots (or I will preferably do both). So expect more technical content as well on this blog. One of the first things I will post soon is about handling Time when deploying to Azure. It will always be bigger picture stuff. For those who are following me via RSS, the URL will change. The new RSS url will be http://eriksteinebach.com/rss2.xml, but because this one is not yet available you can subscribe now to: http://eriksteinebach.github.io/rss2.xml. This one will be active even after I change eriksteinebach.com to the new blog. So if you want a preview of the new blog, you can look at http://eriksteinebach.github.io/, but it is still in development. As I am working as a freelancer, and bills have to be paid so I am always looking for new clients. If you are interested in working with me, please contact me to discus it further.","categories":[],"tags":[{"name":"Housekeeping","slug":"Housekeeping","permalink":"http://eriksteinebach.com/tags/Housekeeping/"}],"keywords":[]},{"title":"The difference between implementing DevOps in small and big teams","slug":"the-difference-between-implementing-devops-in-small-and-big-teams","date":"2014-11-23T18:50:30.000Z","updated":"2018-05-17T23:57:35.511Z","comments":true,"path":"2014/11/23/the-difference-between-implementing-devops-in-small-and-big-teams/","link":"","permalink":"http://eriksteinebach.com/2014/11/23/the-difference-between-implementing-devops-in-small-and-big-teams/","excerpt":"","text":"Jeff Knupp wrote about “How DevOps is killing the developer“. I wanted to respond to this blog post, because you hear this view more often. But I think it stems from a misunderstanding about how DevOps is implemented in different size teams. Thats why I wanted to write about how DevOps should be implemented differently in small and big teams. But lets say first and foremost, DevOps will never kill the developer. How much Dev and Ops responsibilities are performed by the same person depends on the size and maturity of the company, but developers will always exist. The goal of a company should be to get software into production in an efficient manner. This is where DevOps can help. The way to do this is by optimizing the development and operations processes by seeing them as one process instead of two different processes. Achieving this goal can be done in different ways and in my opinion this results in different solutions for different size teams. Small teamsThe DevOps terms stems from startups. DevOps practices are relatively easily incorporated in startups because of their flexible nature, their relatively small teams and the DevOps practices probably aren’t far from the way they were already working. In smaller teams Dev and Ops responsibilities are already assigned to the same person which gives him/her the opportunity to optimize the full process. Most of the time smaller teams (or single programmers) were already doing DevOps without knowing the term. Introducing DevOps might improve on that process and gives them a job title which more resembles their day-to-day work. In companies like this there might not be a “developer” as such anymore. But this will hopefully change when the company grows, and teams get bigger. Bigger teamsWhen DevOps is practiced in bigger teams, DevOps works differently. Instead of sharing different parts of the job, DevOps is much more about communicating what you are doing and understanding what the other guy/girl is doing. And if you know what everybody is doing, it becomes possible to optimize the full process (the end goal). When I look at developers and operators skills, I think of a T shape. The horizontal line stands for their broad knowledge on multiple subjects. The vertical line stands for specialization in their main specialism. For everyone the T will have a different shape. The horizontal and vertical lines will be different lengths for different people. Some people will specialize is a subject, but might not have a very broad view about other subjects (specialists). Some people have a broad view (know a little about everything), but it you go a little deeper into a subject, they are lost (the high level view). If someone focuses more on the vertical or horizontal line depends on your character. A healthy team should have a mix of different people. Implementing DevOps means expending the horizontal line of the T shape for some people. It means that developers need to know more about operations. And that operators need to know more about development. But it doesn’t mean that the developer will do the operators work or the other way around. The members of the team will still have their own specialism (the vertical line) and work inside that specialism. We just try to give them a broader view. For some people this goes against character (they like to focus on their main skill) and this can cause friction and resistance. How do we give people this broader view? Most effective is improving communication between people. An effective way is combining the operations team and the development team. The close proximity results in closer/better communication (if we do it right, but lets assume we did) and it gives the team the freedom to make the necessary changes, because all the stakeholders are represented in the team. The team becomes responsible for the full process and can optimize it. Note that I didn’t say here that one person is responsible. The team with stakeholders from the different disciplines must agree on the optimized process, but all the specialists will focus on implementing their part of the puzzle. Although the way of implementing DevOps might be different, the result should be the same. The result should be an more efficient way of releasing software. This doesn’t mean that the developer “will be killed”, but DevOps makes sure they do a better job, be more efficient and have a clearer understanding of their environment. DevOps makes a developer more professional.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}],"tags":[{"name":"continuous delivery","slug":"continuous-delivery","permalink":"http://eriksteinebach.com/tags/continuous-delivery/"},{"name":"devops","slug":"devops","permalink":"http://eriksteinebach.com/tags/devops/"},{"name":"response","slug":"response","permalink":"http://eriksteinebach.com/tags/response/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}]},{"title":"Introducing Continuous Delivery and DevOps in a company","slug":"introducing-continuous-delivery-and-devops-in-a-company","date":"2014-08-23T18:49:17.000Z","updated":"2018-05-17T23:56:20.775Z","comments":true,"path":"2014/08/23/introducing-continuous-delivery-and-devops-in-a-company/","link":"","permalink":"http://eriksteinebach.com/2014/08/23/introducing-continuous-delivery-and-devops-in-a-company/","excerpt":"","text":"Introducing Continuous Delivery or DevOps should never be a goal in itself, only the means to reach another goal. The definition of the goal depends on the origin for the wish. This could for example be the wish for a smoother deployment process, which would be mainly driven by IT management. Or the reduction of downtime during deployment, mainly driven by business demands (expectation of customers). Continuous Delivery and DevOps can help achieving both of these goals. When a company is looking to introduce Continuous Delivery and/or DevOps it is important to look at the development and deployment processes used inside the company and the organization of different divisions inside the company. Continuous delivery focuses mostly on the deployment pipeline, the tools. DevOps focuses more on the organization of the company. One important aspect is looking at responsibility. Who is responsible when something goes wrong? Less important is who is theoretically responsible, but who is really responsible, so who gets the call when a system goes down at 2AM? Who has to deal with the fallout when something breaks? Here you will find the smells that show where the problems are inside a company and where DevOps could create a better culture. Another aspect is looking at how software gets deployed in production. Part of introducing CD is automating the deployment process. The level of automation will depend on the investment that the company is willing to make. Currently the tooling available is not yet at such a level that facebook or google style “unnoticed” deployment is easily possible. I do expect this to get better in the future (look at Microsoft’s Azure for example), but there will always be an extra cost to doing CD. Think for example about higher complexity in database design. Companies will have to decide the investment they are willing to make and where they see the most benefit. For some companies a Facebook of Google style deployment will not make business sense. Lets look a company developing B2B applications for Dutch clients and their need for CD. They may still want rapid deployment (for example every week) to support there Agile development process. But their application is only used during business hours. After business hours there is sometimes an occasional user (someone working late or forgot something and does it from home), but 99% of the activity is during the day. How much will an investment in live updating of the application be worth? If we can do the deployment in 15 minutes and do it always outside of business hours? That would probably be good enough, right? It is important to realize though, that there are always some side scenario’s to consider. Think for example in the above scenario about this nasty bug you found that needs to be fixed immediately. Your patch process is (if you follow CD) the same as your normal deployment process. So now we have to bring our application down during business hours. This is probably not different than before implementing CD, but these gotcha’s have to be clear for management. A cost-benefit evaluation should have been made with every scenario in mind and agreed upon by IT &amp; business management. Only than can be decided how much the introduction of Continuous Delivery and DevOps inside a company may cost. In future posts I will go into more detail about parts mentioned in this blog post.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}],"tags":[{"name":"continuous delivery","slug":"continuous-delivery","permalink":"http://eriksteinebach.com/tags/continuous-delivery/"},{"name":"devops","slug":"devops","permalink":"http://eriksteinebach.com/tags/devops/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}]},{"title":"How to explain Continuous Delivery to backpackers","slug":"how-to-explain-continuous-delivery-to-backpackers","date":"2014-08-06T18:47:44.000Z","updated":"2018-05-17T23:56:12.473Z","comments":true,"path":"2014/08/06/how-to-explain-continuous-delivery-to-backpackers/","link":"","permalink":"http://eriksteinebach.com/2014/08/06/how-to-explain-continuous-delivery-to-backpackers/","excerpt":"","text":"The last months I have been traveling Southeast Asia as a backpacker. During my travels I met a lot of other travelers. The conversations I have with them, tend to all start in a similar way. Where have you been? Where are you going next? How long do you have to travel? And when they hear that I’m 30, the question that often comes up is: What did you do before you went backpacking? I was helping a company make the change to Continuous Delivery. This caused a problem for me. How do I describe Continuous Delivery to a backpacker who uses Facebook to connect with fellow travelers and Skype to call home, but has no knowledge of software development? I told them I was a IT consultant, which is so vague it razes even more questions. I could say I was a programmer, which was true a couple of years ago, but not really what I have been doing the last year I was working. This is how I tried to explain it: Look at Google and Facebook, you expect there website to be available. When you want to post your instagram picture you expect it to work. But even Google and Facebook need to update their website. They do it in a way so that the user doesn’t notice (he/she only sees the change) and that is what users are starting to expect from websites of applications. Now at least in the Netherlands, a lot of companies understand this, but do not know how to change there process to make this possible. Introducing Continuous Delivery (supported by DevOps) in a company is a way to achieve this Google or Facebook like updating for every company. Those who know Continuous Delivery will say that this is a extreme oversimplification of what is Continuous Delivery is. This is true, but how do I explain software quality improvements, a quick feedback loop for customers and fewer deployment issues and risks to fellow travelers? And do the customers from the company really notice these changes? Hopefully it wasn’t so bad before that this would be the case. So although this is great for developers, operations and the IT manager, does the business really care? As long as the website doesn’t go down regularly and the developers can keep up with the requested changes, the business doesn’t really care. This is my first blogpost about Continuous Delivery. My goal is to go deeper into different aspects of Continuous Delivery and DevOps, because I believe they are an integral part of the future of software development.","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}],"tags":[{"name":"continuous delivery","slug":"continuous-delivery","permalink":"http://eriksteinebach.com/tags/continuous-delivery/"},{"name":"devops","slug":"devops","permalink":"http://eriksteinebach.com/tags/devops/"},{"name":"backpacking","slug":"backpacking","permalink":"http://eriksteinebach.com/tags/backpacking/"}],"keywords":[{"name":"DevOps","slug":"DevOps","permalink":"http://eriksteinebach.com/categories/DevOps/"}]}]}